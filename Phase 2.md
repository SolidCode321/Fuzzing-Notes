Created a basic random fuzzer:

run_fuzzer_pipeline.py:
```
# run_fuzzer_pipeline.py
from fuzzers.random_fuzzer import generate_random_string, save_to_file
from fuzzers.clamav_scanner import scan_with_clamav
import csv
import os
  
# Prepare logs directory
logfile = 'logs/scan_results.csv'
os.makedirs('logs', exist_ok=True)

# Set up CSV for logging results
with open(logfile, 'w', newline='') as csvfile:
fieldnames = ['file', 'infected', 'crashed', 'returncode', 'stdout', 'stderr']
writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
writer.writeheader()

# Fuzz and scan loop
for i in range(10): # Adjust number of iterations as needed
fuzz = generate_random_string(200)
file_path = save_to_file(fuzz) # Saves to ../samples/fuzzed_*.txt
result = scan_with_clamav(file_path)

infected = 'FOUND' in result.get('stdout', '')
crashed = result.get('returncode', 0) != 0

writer.writerow({
'file': file_path,
'infected': infected,
'crashed': crashed,
'returncode': result.get('returncode'),
'stdout': result.get('stdout'),
'stderr': result.get('stderr')
})

print(f"[{i+1}] Scanned: {file_path} | Infected: {infected} | Crashed: {crashed}")
```

random_fuzzer.py:
```
# fuzzers/random_fuzzer.py
import os
import random
import string

def generate_random_string(length=100):
return ''.join(random.choices(string.printable, k=length))

def save_to_file(content, directory=os.path.join(os.pardir, 'samples/fuzzed'), extension='.txt'):
os.makedirs(directory, exist_ok=True)
file_path = os.path.join(directory, f'fuzz_{random.randint(1000,9999)}{extension}')
with open(file_path, 'w') as f:
f.write(content)
return file_path

if __name__ == "__main__":
for _ in range(10): # Generate 10 samples
fuzz = generate_random_string()
path = save_to_file(fuzz)
print(f"Generated: {path}")
```

clamav_scanner.py:
```
# fuzzers/clamav_scanner.py
import subprocess

def scan_with_clamav(file_path):
result = subprocess.run(
['clamscan', '--no-summary', '--infected', file_path],
capture_output=True,
text=True
)
return {
'file': file_path,
'stdout': result.stdout.strip(),
'stderr': result.stderr.strip(),
'returncode': result.returncode
}

if __name__ == "__main__":
import sys
if len(sys.argv) < 2:
print("Usage: python clamav_scanner.py <file_path>")
else:
result = scan_with_clamav(sys.argv[1])
print(result)
```

### But This is too Slow
After implementing a random fuzzer, I noticed that running clamscan on each file took about 15-20 seconds, which was not ideal.

So I looked into why this took so long.
The main reason why this took too long is that each time a file was generated a separate clamscan command memory and loaded a dictionary of malicious files which it checks to notice any similar patterns. This process repeats for each file, resulting in a massive increase in time taken.

#### Fixes:
There is a ClamAV-daemon project which loads the dictionary once and keeps it in the memory for as long as we choose to do so.

This allows s to save on the memory load time by a lot.

To do this we first need to create a [[Clamavd]] docker container to host daemon.

Our end goal is shown in the below flowchart:
![[Untitled.jpg]]


After implementing this the final code made is:

faster_fuzzer_pipeline.py
```
# faster_fuzzer_pipeline.py

import os
import csv
import time
import clamd
from fuzzers.random_fuzzer import generate_random_string, save_to_file

# Connect to clamd daemon
cd = clamd.ClamdNetworkSocket(host='localhost', port=3310)

# Ensure logs directory exists
log_dir = 'FuzzingAV-prototypes/logs'
os.makedirs(log_dir, exist_ok=True)
logfile = os.path.join(log_dir, 'scan_results_fast.csv')

# Prepare CSV file
with open(logfile, 'w', newline='') as csvfile:
    fieldnames = ['file', 'infected', 'result']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

    start_time = time.time()  # Start timing

    for _ in range(100000):  # Adjust number of iterations
        fuzz = generate_random_string(200)
        file_path = save_to_file(fuzz)  # saves to ../samples/fuzzed_xx

        try:
            result = cd.scan(file_path)
            if result and file_path in result:
                scan_result = result[file_path]
                infected = scan_result[0] == 'FOUND'
                output = scan_result[1]
            else:
                infected = False
                output = 'Clean or Unreadable'
        except Exception as e:
            infected = False
            output = f"Error: {str(e)}"

        writer.writerow({
            'file': file_path,
            'infected': infected,
            'result': output
        })

        print(f"Scanned: {file_path}, Infected: {infected}, Result: {output}")

    end_time = time.time()  # End timing
    elapsed = end_time - start_time
    print(f"\n‚è±Ô∏è Total scan time for 100 files: {elapsed:.2f} seconds")
    print(f"üìÑ Results saved to: {logfile}")
```

For about 100K files the output was:
```
Scanned: ../samples/fuzzed_01, Infected: False, Result: Clean or Unreadable
...
...
...

‚è±Ô∏è Total scan time for 100 files: 154.09 seconds
üìÑ Results saved to: FuzzingAV-prototypes/logs/scan_results_fast.csv
```

154 seconds for 100K file is a drastic improvement! üëç

